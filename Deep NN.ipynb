{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Desired output matrix Y[2x3] =\n",
      " [[0.126 0.207 0.051]\n",
      " [0.441 0.03  0.457]]\n",
      "--------------------------------------\n",
      "Cost = 0.03998670504863828\n",
      "Output = [[0.118 0.14  0.125]\n",
      " [0.363 0.245 0.324]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "def display_init (X, Y, layer_dims, m, learning_rate, no_iter, W, b):\n",
    "    print ('--------------------------------------')\n",
    "#     print ('Inputs per sample = {}'.format(layer_dims[0]))\n",
    "#     print ('Number of samples = {}'.format(m))\n",
    "#     print ('Outputs per sample = {}'.format(layer_dims[-1]))\n",
    "#     print ('Number of hidden layers = {}'.format(len(layer_dims)-2))\n",
    "#     print ('Number of units in each layer = {}'.format(layer_dims))\n",
    "#     print ('Learning rate = {}'.format(learning_rate))\n",
    "#     print ('Number of iterations = {}'.format(no_iter))\n",
    "#     print ('Input matrix X[{}x{}] =\\n {}'.format(layer_dims[0], m, X))\n",
    "    print ('Desired output matrix Y[{}x{}] =\\n {}'.format(layer_dims[-1], m, Y))\n",
    "#     print ('Initial weights W =\\n')\n",
    "#     [print ('W[{}] {} = {},\\n'.format(i, W[i].shape, W[i])) for i in range (len(layer_dims)-1)]\n",
    "#     print ('Initial biases b =\\n')\n",
    "#     [print ('b[{}] {} = {},\\n'.format(i, b[i].shape, b[i])) for i in range (len(layer_dims)-1)]\n",
    "    print ('--------------------------------------')\n",
    "\n",
    "def test_init():\n",
    "    X = np.array([[0.05], [0.10]])      # Inputs\n",
    "    W = np.array([[[0.15,0.20], [0.25,0.30]],[[0.40,0.45], [0.50,0.55]]])\n",
    "    b = np.array([[0.35],[0.60]])\n",
    "    Y = np.array([[0.01], [0.99]])      # Desired output\n",
    "    return(X,W,b,Y)\n",
    "\n",
    "def initialize_param(layer_dims):\n",
    "    W = []\n",
    "    b = []\n",
    "    for i in range(1,len(layer_dims)):\n",
    "        tmp = np.random.randn(layer_dims[i],layer_dims[i-1])*0.01\n",
    "        W.append(np.random.randn(layer_dims[i],layer_dims[i-1])*0.01)\n",
    "        b.append(np.zeros((layer_dims[i],1)))\n",
    "    return (W,b)\n",
    "\n",
    "def activation(act_type, z):\n",
    "    if act_type == 'sigmoid':\n",
    "        return(1/(1 + np.exp(-z)))\n",
    "    elif act_type == 'relu':\n",
    "        return(np.maximum(z, 0))\n",
    "\n",
    "def forward_prop(X, W, b, Y, layers):\n",
    "    Z = []\n",
    "    A = []\n",
    "    Z_prev = X\n",
    "    for l in range(layers):     # Computing Z & A across each layer & storing the results in a list\n",
    "        Z.append(np.dot(W[l],Z_prev)+b[l])\n",
    "        A.append(activation('sigmoid',Z[l]))\n",
    "        Z_prev = A[l]\n",
    "    Z = np.array(Z)\n",
    "    A = np.array(A)\n",
    "    return(Z,A)\n",
    "\n",
    "def cost_func (AL, Y, m, cf_type):\n",
    "    if (cf_type == 'cross_entropy'):\n",
    "        cost = (-1 / m) * np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),np.log(1-AL)))\n",
    "        cost = np.squeeze(cost)\n",
    "    elif (cf_type == 'sq_err'):\n",
    "        cost = np.sum(1/2*np.square(Y - AL))\n",
    "    return(cost)\n",
    "\n",
    "        \n",
    "def cost_func_cross_entropy (AL,Y,m):\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    return(cost)\n",
    "\n",
    "def cost_func_sq_err (AL,Y,m):\n",
    "    cost = np.sum(1/2*np.square(Y - AL))\n",
    "    return(cost)\n",
    "\n",
    "def back_prop(A,Y,m,layers):\n",
    "    dZ = []\n",
    "    dW = []\n",
    "    db = []\n",
    "    dZ.insert(0,np.array(A[-1]-Y))\n",
    "    dW.insert(0,(1 / m)*np.dot(dZ[0],A[-2].T))\n",
    "    db.insert(0,(1 / m)*np.sum(dZ[0], axis=1, keepdims=True))\n",
    "    for l in range(layers-2,-1,-1):\n",
    "#         dZ.insert(0,np.dot(dW[0].T,dZ[0])*(A[l-1]*(1-A[l-1])))\n",
    "        dZ_tmp1 = np.dot(dW[0].T,dZ[0])\n",
    "        dZ_tmp2 = np.dot(A[l-1].T,(1-A[l-1]))\n",
    "        dZ.insert(0,np.dot(dZ_tmp1, dZ_tmp2))\n",
    "        dW.insert(0,np.dot(dZ[0],A[l-1].T))\n",
    "        db.insert(0,(1 / m)*np.sum(dZ[0], axis=1, keepdims=True))\n",
    "    return(dW, db)    \n",
    "        \n",
    "def adjust_weights_bias(W, b, dW, db, learning_rate):\n",
    "    W = W - np.multiply(learning_rate,dW)\n",
    "    b = b - np.multiply(learning_rate,db)\n",
    "    return (W, b)\n",
    "    \n",
    "def main():\n",
    "    ##### Initialize Neural Network #####\n",
    "    np.random.seed(3)\n",
    "    layer_dims = [2,5,2]     # no. of units in each layer\n",
    "    m = 3     # no. of samples\n",
    "    learning_rate = 0.5\n",
    "    no_iter = 10000\n",
    "    X = np.random.rand(layer_dims[0],m)      # Input set across m samples\n",
    "    Y = np.random.rand(layer_dims[-1],m)      # Desired output across m samples\n",
    "    (W, b) = initialize_param(layer_dims)\n",
    "#     (X,W,b,Y) = test_init()     # Initialize with https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ \n",
    "    display_init (X, Y, layer_dims, m, learning_rate, no_iter, W, b)     # Print out initialized parameters\n",
    "    \n",
    "    for iter in range(no_iter):\n",
    "        ##### Forward Propagation #####\n",
    "        (Z,A) = forward_prop (X, W, b, Y, layers=(len(layer_dims)-1))\n",
    "\n",
    "        ##### Calculate Cost #####\n",
    "#         cost = cost_func(A[-1],Y,m,cf_type='cross_entropy')\n",
    "        cost = cost_func(A[-1],Y,m,cf_type='sq_err')\n",
    "#         print ('Cost = {}'.format(cost))\n",
    "\n",
    "        ##### Back Propagation #####\n",
    "        (dW, db) = back_prop(A,Y,m,layers=(len(layer_dims)-1))\n",
    "        \n",
    "        ##### Adjust Weights\n",
    "        (W, b) = adjust_weights_bias(W, b, dW, db, learning_rate)\n",
    "\n",
    "#     print ('Adjusted weights = {}'.format(W))\n",
    "#     print ('Adjusted bias = {}'.format(b))\n",
    "    print ('Cost = {}'.format(cost))\n",
    "    print ('Output = {}'.format(A[-1]))\n",
    "######################################################\n",
    " \n",
    "if __name__== \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 1024, 3)\n",
      "---------\n",
      "# of pixels =  786432\n",
      "First pixel @ (0,0) =  [215 218 223]\n",
      "Last pixel @ (767, 1023) =  [163 152 148]\n",
      "---------\n",
      "#########\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    " \n",
    "img = Image.open(\"../data_misc/IMG_2053.JPG\")\n",
    "arr = np.array(img)\n",
    "\n",
    "print (arr.shape)\n",
    "print ('---------')\n",
    "print ('# of pixels = ', arr.shape[0]*arr.shape[1])\n",
    "print ('First pixel @ (0,0) = ', arr[0,0])\n",
    "print ('Last pixel @ (767, 1023) = ', arr[767,1023])\n",
    "print ('---------')\n",
    "arr_flat = arr.reshape(1,-1)\n",
    "print ('#########')\n",
    "print ('---------')\n",
    "# print ('Red = {}'.format(arr[:,:,0]))\n",
    "# print ('Green = {}'.format(arr[:,:,1]))\n",
    "# print ('Blue = {}'.format(arr[:,:,2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
